{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3beb7a66-40c5-4035-8b2b-79f5467925f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from rouge import Rouge\n",
    "from nltk.translate import bleu_score as bleu_scorer\n",
    "from nltk.translate import meteor_score as nltk_meteor_scorer\n",
    "import nltk.translate.gleu_score as gleu_scorer\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a0736e9-8320-46df-98b7-1acfa8c1d216",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    EncoderDecoderModel,\n",
    "    AutoTokenizer,\n",
    "    PegasusTokenizer,\n",
    "    # PegasusXForConditionalGeneration,\n",
    "    HfArgumentParser,\n",
    "    MBartTokenizer,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed,\n",
    "    # MvpTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fa7817-55c7-49ac-8bff-02d15e3b6bbf",
   "metadata": {},
   "source": [
    "# Code for NLU&POL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f76ea61-7194-44b8-bcc3-bb2b4011d666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sp_tokens(txt):\n",
    "    # words = txt.strip().split()\n",
    "    sp_tokens = [\n",
    "        '[CLS]',\n",
    "        '[PAD]',\n",
    "        '[SEP]',\n",
    "        \"[unused\",\n",
    "        '__start__',\n",
    "        '__null__',\n",
    "        '__end__',\n",
    "        '<pad>',\n",
    "        '</s>',\n",
    "        '<s>',\n",
    "        '.',\n",
    "        '\u0017','�','\u0006','\u0005','\u0018',\n",
    "    ]\n",
    "   \n",
    "    \n",
    "    new_text = txt\n",
    "    for e in sp_tokens:\n",
    "        new_text = new_text.replace(e,\" \")\n",
    "    \n",
    "   \n",
    "        # new_text = new_text.replace(e,f\" {e} \")\n",
    "    new_text = new_text.replace(']',\"] \").replace('[',\" [\")\n",
    "    words = new_text.strip().split()\n",
    "    words = [e for e in words if e != \"\"]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def remove_puncs(txt):\n",
    "    puncs = [\n",
    "        '!', '\"', '#', '$', '%', '&', \"'\", '(', \n",
    "        ')', '*', '+', ',', '-', '.', '/', ':',\n",
    "        ';', '<', '=', '>', '?', '@', r'\\\\', r'^'\n",
    "        '`', '{', '|', '}', '~',\n",
    "    ]\n",
    "    new_text = txt \n",
    "    for e in puncs:\n",
    "        new_text = new_text.replace(e,\" \")\n",
    "    words = new_text.strip().split()\n",
    "    words = [e for e in words if e != \"\"]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def remove_dup_tokens(txt):\n",
    "    words = txt.strip().split()\n",
    "    new_words = []\n",
    "    for e in words:\n",
    "        if new_words == [] or e != new_words[-1]:\n",
    "            new_words.append(e)\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "def check_key(t):\n",
    "    return t.startswith('[') and t.endswith(']')\n",
    "\n",
    "def process_single(text):\n",
    "    # test_text = \"QAB [poi] Chevron [poi] HR [exit]\"\n",
    "    words = text.split()\n",
    "    data = []\n",
    "    for word in words:\n",
    "        is_key = check_key(word)\n",
    "        if not is_key and data == []:\n",
    "            continue\n",
    "        elif is_key:\n",
    "            data.append([word])\n",
    "        else:\n",
    "            data[-1].append(word)\n",
    "    data = [e for e in data if len(e)>=2]\n",
    "    keys = []\n",
    "    values = []\n",
    "    kvs = []\n",
    "    for e in data:\n",
    "        k = e[0]\n",
    "        v = ' '.join(e[1:])\n",
    "        kv = f\"{k}-{v}\"\n",
    "        # if kv in kvs:\n",
    "        #     continue\n",
    "        keys.append(k)\n",
    "        values.append(v)\n",
    "        kvs.append(kv)\n",
    "    return {\n",
    "        \"data\": data,\n",
    "        \"keys\": keys,\n",
    "        \"values\": values,\n",
    "        \"combined_kv\": kvs,\n",
    "    }\n",
    "\n",
    "\n",
    "def diff_prediction_source(pred, sour):\n",
    "    '''\n",
    "    a = [\"a\",\"c\",\"b\",\"v\",\"c\",\"b\",\"b\"]\n",
    "    b = [\"a\",\"z\",\"c\",\"b\",\"b\",\"c\",\"c\",\"c\"]\n",
    "\n",
    "    overlap:    [\"a\",\"c\",\"b\",\"c\",\"b\"]\n",
    "    a_not_in_b: [\"v\",\"b\"]\n",
    "    b_not_in_a: [\"z\",\"c\",\"c\"]\n",
    "    '''\n",
    "    a = pred\n",
    "    b = sour\n",
    "    count_a = Counter(a)\n",
    "    count_b = Counter(b)\n",
    "\n",
    "    # ab中都有\n",
    "    both_dict = {}\n",
    "    for k,t1 in count_a.items():\n",
    "        t2 = count_b.get(k, 0)\n",
    "        times = min(t1,t2)\n",
    "        if times > 0:\n",
    "            both_dict[k] = times\n",
    "    both_list = [[k]*v for k,v in both_dict.items()]\n",
    "    both_list = sum(both_list,[])\n",
    "\n",
    "    # a有b没有\n",
    "    a_not_in_b_dict = {}\n",
    "    for k, t1 in count_a.items():\n",
    "        t = both_dict.get(k,0)\n",
    "        diff_t = t1-t\n",
    "        if diff_t > 0:\n",
    "            a_not_in_b_dict[k] = diff_t\n",
    "    a_not_in_b_list = [[k]*v for k,v in a_not_in_b_dict.items()]\n",
    "    a_not_in_b_list = sum(a_not_in_b_list,[])\n",
    "\n",
    "    # b有a没有\n",
    "    b_not_in_a_dict = {}\n",
    "    for k, t2 in count_b.items():\n",
    "        t = both_dict.get(k,0)\n",
    "        diff_t = t2-t\n",
    "        if diff_t > 0:\n",
    "            b_not_in_a_dict[k] = diff_t\n",
    "    b_not_in_a_list = [[k]*v for k,v in b_not_in_a_dict.items()]\n",
    "    b_not_in_a_list = sum(b_not_in_a_list,[])\n",
    "    \n",
    "    return both_list,a_not_in_b_list,b_not_in_a_list\n",
    "\n",
    "def eval_nlu_and_pol(predictions,targets):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    # tn = 0\n",
    "    fn = 0\n",
    "\n",
    "\n",
    "    for pred, targ in zip(predictions, targets):\n",
    "        pred_d = process_single(pred)\n",
    "        pred_kv = pred_d['combined_kv']\n",
    "\n",
    "        targ_d = process_single(targ)\n",
    "        targ_kv = targ_d['combined_kv']\n",
    "        \n",
    "        \n",
    "        if pred_kv == [] and targ_kv == []:\n",
    "            tp += 1\n",
    "            continue\n",
    "\n",
    "#         pred_correct = [e for e in pred_kv if e in targ_kv]\n",
    "#         pred_wrong = [e for e in pred_kv if e not in targ_kv]\n",
    "\n",
    "#         targ_correct = [e for e in targ_kv if e in pred_kv]\n",
    "#         targ_wrong = [e for e in targ_kv if e not in pred_kv]\n",
    "\n",
    "        pred_correct,pred_wrong,targ_wrong = diff_prediction_source(pred_kv,targ_kv)\n",
    "        \n",
    "        tp += len(pred_correct)\n",
    "        fp += len(pred_wrong)\n",
    "        fn += len(targ_wrong)\n",
    "    # acc = correct / (correct + wrong + 1e-10)\n",
    "    # print(f\"ACC: {acc*100:.2f}%\")    \n",
    "\n",
    "\n",
    "\n",
    "    precision = tp / (tp + fp + 1e-10)\n",
    "    recall = tp / (tp + fn + 1e-10)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-10)\n",
    "\n",
    "    # print(f\"Precision: {precision*100:.2f}%\")\n",
    "    # print(f\"Recall:    {recall*100:.2f}%\")\n",
    "    # print(f\"F1:        {f1*100:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "    }\n",
    "\n",
    "def eval__nlu_and_pol_by_fname(pred_f,targ_f,remove_sp_token=True,remove_punc=True,lower=True,remove_dup_token=False):\n",
    "    result_fpath = Path(pred_f).absolute().resolve()\n",
    "    target_fpath = Path(targ_f).absolute().resolve()\n",
    "    with result_fpath.open() as f:\n",
    "        predictions = f.read().strip().splitlines()\n",
    "    with target_fpath.open() as f:\n",
    "        targets = f.read().strip().splitlines()\n",
    "        \n",
    "    if remove_sp_token:\n",
    "        predictions = [remove_sp_tokens(e) for e in predictions]\n",
    "        targets = [remove_sp_tokens(e) for e in targets]\n",
    "    if remove_punc:\n",
    "        predictions = [remove_puncs(e) for e in predictions]\n",
    "        targets = [remove_puncs(e) for e in targets]\n",
    "    if lower:\n",
    "        predictions = [e.lower() for e in predictions]\n",
    "        targets = [e.lower() for e in targets]\n",
    "    if remove_dup_token:\n",
    "        predictions = [remove_dup_tokens(e) for e in predictions]\n",
    "        targets = [remove_dup_tokens(e) for e in targets]\n",
    "    \n",
    "    results = eval_nlu_and_pol(predictions,targets)\n",
    "    return results\n",
    "\n",
    "def print_results(results, modelname=None):\n",
    "    p = results['precision']\n",
    "    r = results['recall']\n",
    "    f = results['f1']\n",
    "    if modelname is not None:\n",
    "        print(f\"Model: {modelname}\")\n",
    "    print(f\"Precision: {p*100:.2f}\")\n",
    "    print(f\"Recall:    {r*100:.2f}\")\n",
    "    print(f\"F1:        {f*100:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be981f1-713e-4a85-8637-4b58bc8e18fd",
   "metadata": {},
   "source": [
    "# Code for NLG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8b815d8-7505-4f0d-8574-99e6c1eb8b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_score_pip(answers,refs):\n",
    "    '''\n",
    "    https://pypi.org/project/rouge/\n",
    "    '''\n",
    "    r = Rouge()\n",
    "    # rouge_result = r.get_scores([\"how are you?\"],[\"how are you!\"])\n",
    "    rouge_result = r.get_scores(answers,refs,avg=True)\n",
    "    return rouge_result['rouge-l']['f']*100\n",
    "\n",
    "def bleu_n_scores(answers,refs,n=4):\n",
    "    sm_function = bleu_scorer.SmoothingFunction().method0\n",
    "    w = 1/n\n",
    "    weights = [w for _ in range(n)]\n",
    "    answers_words = [e.split() for e in answers]\n",
    "    refs_words = [e.split() for e in refs]\n",
    "    list_of_refs = [[e] for e in refs_words]\n",
    "    blue_n_score = bleu_scorer.corpus_bleu(\n",
    "        list_of_refs,\n",
    "        answers_words,\n",
    "        weights=weights,\n",
    "\n",
    "    )\n",
    "    return blue_n_score*100\n",
    "\n",
    "def meteor_score_by_nltk(answers,refs):\n",
    "    meteor_scores = []\n",
    "    for ans,ref in zip(answers,refs):\n",
    "        cur_score = nltk_meteor_scorer.meteor_score([ref.split()],ans.split())\n",
    "        meteor_scores.append(cur_score)\n",
    "    meteor_score = np.mean(meteor_scores)\n",
    "    return meteor_score*100\n",
    "\n",
    "def gleu_score_by_nltk(answers,refs):\n",
    "    ref_words = [[e.split()] for e in refs]\n",
    "    ans_words = [e.split() for e in answers]\n",
    "    gleu_score = gleu_scorer.corpus_gleu(ref_words, ans_words)\n",
    "    return gleu_score*100\n",
    "\n",
    "def eval_nlg(predictions,targets):\n",
    "    prediction_nonempty = []\n",
    "    target_nonempty = []\n",
    "    for pred,targ in zip(predictions,targets):\n",
    "        if pred.strip() == \"\" or targ.strip() == \"\":\n",
    "            continue\n",
    "        prediction_nonempty.append(pred)\n",
    "        target_nonempty.append(targ)\n",
    "    \n",
    "    bleu4_score = bleu_n_scores(prediction_nonempty,target_nonempty,n=4)\n",
    "    rouge_score = rouge_score_pip(prediction_nonempty,target_nonempty)\n",
    "    meteor_score = meteor_score_by_nltk(prediction_nonempty,target_nonempty)\n",
    "    gleu_score = gleu_score_by_nltk(prediction_nonempty,target_nonempty)\n",
    "    return {\n",
    "        \"BLEU-4\": bleu4_score,\n",
    "        \"ROUGE-L\": rouge_score,\n",
    "        \"METEOR\": meteor_score,\n",
    "        \"GLEU\": gleu_score,\n",
    "    }\n",
    "\n",
    "def eval_nlg_by_fname(pred_f,targ_f,remove_sp_token=True,remove_punc=True,lower=True,remove_dup_token=False):\n",
    "    # remove_dup_token for nothing\n",
    "    result_fpath = Path(pred_f).absolute().resolve()\n",
    "    target_fpath = Path(targ_f).absolute().resolve()\n",
    "\n",
    "    with result_fpath.open() as f:\n",
    "        predictions = f.read().strip().splitlines()\n",
    "    \n",
    "\n",
    "    with target_fpath.open() as f:\n",
    "        targets = f.read().strip().splitlines()\n",
    "    \n",
    "    if remove_sp_token:\n",
    "        predictions = [remove_sp_tokens(e) for e in predictions]\n",
    "        targets = [remove_sp_tokens(e) for e in targets]\n",
    "    if remove_punc:\n",
    "        predictions = [remove_puncs(e) for e in predictions]\n",
    "        targets = [remove_puncs(e) for e in targets]\n",
    "    if lower:\n",
    "        predictions = [e.lower() for e in predictions]\n",
    "        targets = [e.lower() for e in targets]\n",
    "    \n",
    "    results = eval_nlg(predictions,targets)\n",
    "    return results\n",
    "\n",
    "\n",
    "def print_nlg_results(results, modelname=None):\n",
    "    b = results['BLEU-4']\n",
    "    r = results['ROUGE-L']\n",
    "    m = results['METEOR']\n",
    "    g = results['GLEU']\n",
    "    if modelname is not None:\n",
    "        print(f\"Model: {modelname}\")\n",
    "    print(f\"BLEU-4:  {b:.2f}\")\n",
    "    print(f\"ROUGE-L: {r:.2f}\")\n",
    "    print(f\"METEOR:  {m:.2f}\")\n",
    "    print(f\"GLEU:    {g:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0a75e747-7299-427d-a263-00731d2a8f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode: history\n",
      "task: NLU\n",
      "\n",
      "Precision: 88.94\n",
      "Recall:    91.36\n",
      "F1:        90.13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NLU&POL  我们的模型\n",
    "model = \"gpt2\"\n",
    "\n",
    "mode = \"history\"\n",
    "# mode = \"profile\"\n",
    "# mode = \"history-profile\"\n",
    "\n",
    "\n",
    "task = \"NLU\"\n",
    "# task = \"POL\"\n",
    "\n",
    "\n",
    "# if_bs = False\n",
    "\n",
    "# if_ori = False\n",
    "if_ori = True\n",
    "ori = \"ori-\" if if_ori else \"\"\n",
    "\n",
    "if_bs = True\n",
    "bs = \"-bs\" if if_bs else \"\"\n",
    "\n",
    "fpath = Path(f'./inference-results/{ori}{mode}-{model}{bs}/').absolute().resolve()\n",
    "# if if_bs:\n",
    "#     fpath = Path(f'./inference-results/{mode}-{model}-bs/').absolute().resolve()\n",
    "# else:\n",
    "#     fpath = Path(f'./inference-results/{mode}-{model}/').absolute().resolve()\n",
    "\n",
    "\n",
    "\n",
    "print(f\"mode: {mode}\")\n",
    "print(f\"task: {task}\\n\")\n",
    "dset = \"test\"\n",
    "# dset = \"dev\"\n",
    "result_fpath = fpath.joinpath(f\"{dset}-{task}.result\")\n",
    "target_fpath = fpath.joinpath(f\"{dset}-{task}.target\")\n",
    "\n",
    "\n",
    "# result_fpath = Path('/home/jitianbo/Workspace/driver_simulator_kvret/dialog-task/predictions/NLU-bart-large-test.txt')\n",
    "# result_fpath = Path('/home/jitianbo/Workspace/driver_simulator_kvret/dialog-task/predictions/NLU-t5-large-test.txt')\n",
    "# target_fpath = Path('/home/jitianbo/Workspace/driver_simulator_kvret/dialog-task/NLU/test.target')\n",
    "\n",
    "# result_fpath = Path('/home/jitianbo/Workspace/driver_simulator_kvret/dialog-task/predictions/DST-bart-large-test.txt')\n",
    "# result_fpath = Path('/home/jitianbo/Workspace/driver_simulator_kvret/dialog-task/predictions/DST-t5-large-val.txt')\n",
    "# target_fpath = Path('/home/jitianbo/Workspace/driver_simulator_kvret/dialog-task/DST/test.target')\n",
    "\n",
    "with result_fpath.open() as f:\n",
    "    predictions = f.read().strip().splitlines()\n",
    "with target_fpath.open() as f:\n",
    "    targets = f.read().strip().splitlines()\n",
    "# assert len(predictions) == len(targets)\n",
    "results = eval__nlu_and_pol_by_fname(result_fpath,target_fpath,remove_dup_token=False)\n",
    "print_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "af177ca9-0f35-4272-a527-a0b7bb948c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: /home/jitianbo/Workspace/driver_simulator_kvret/simulator/ablation/inference/inference-results/ori-history-gpt2-bs\n",
      "BLEU-4:  27.82\n",
      "ROUGE-L: 48.67\n",
      "METEOR:  48.43\n",
      "GLEU:    27.41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NLG 我们的模型\n",
    "mode = \"history\"\n",
    "# mode = \"profile\"\n",
    "# mode = \"history-profile\"\n",
    "model = \"gpt2\"\n",
    "\n",
    "# if_bs = False\n",
    "if_ori = True\n",
    "ori = \"ori-\" if if_ori else \"\"\n",
    "\n",
    "if_bs = True\n",
    "bs = \"-bs\" if if_bs else \"\"\n",
    "\n",
    "fpath = Path(f'./inference-results/{ori}{mode}-{model}{bs}/').absolute().resolve()\n",
    "\n",
    "task = \"NLG\"\n",
    "dset = \"test\"\n",
    "# dset = \"dev\"\n",
    "result_fpath = fpath.joinpath(f\"{dset}-{task}.result\")\n",
    "target_fpath = fpath.joinpath(f\"{dset}-{task}.target\")\n",
    "\n",
    "\n",
    "results = eval_nlg_by_fname(result_fpath,target_fpath)\n",
    "print_nlg_results(results, modelname=str(fpath))\n",
    "\n",
    "# result_fpath = Path('/home/jitianbo/Workspace/driver_simulator_kvret/dialog-task/predictions/NLG-t5-large-test.txt')\n",
    "# result_fpath = Path('/home/jitianbo/Workspace/driver_simulator_kvret/dialog-task/predictions/DST-t5-large-val.txt')\n",
    "# target_fpath = Path('/home/jitianbo/Workspace/driver_simulator_kvret/dialog-task/NLG/test.target')\n",
    "\n",
    "\n",
    "# with result_fpath.open() as f:\n",
    "#     predictions = f.read().strip().splitlines()\n",
    "# with target_fpath.open() as f:\n",
    "#     targets = f.read().strip().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c7d2fe-bd65-4b41-bcba-2a4dead8729d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
