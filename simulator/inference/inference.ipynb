{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c02224-6d74-4c84-9489-62ac90e34348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import os,json\n",
    "from dataclasses import dataclass\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    MODEL_FOR_CAUSAL_LM_MAPPING,\n",
    "    pipeline,\n",
    ")\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import datasets\n",
    "from datasets import load_dataset, load_metric\n",
    "# from sysproxy import SysProxy\n",
    "# sys_proxy = SysProxy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfc2a0c-21ba-484c-99c6-a0f972a23cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(modelpath,device='cuda'):\n",
    "    # config = AutoConfig.from_pretrained(modelname)\n",
    "    model = AutoModelForCausalLM.from_pretrained(modelpath).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(modelpath)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04560304-95e9-4f42-9447-80df7a6103dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taskname = \"NLG\"\n",
    "# taskname = \"NLU\"\n",
    "# source_fname = f'/home/jitianbo/Workspace/driver_simulator_kvret/data/data_for_clm/{taskname}/test-{taskname}.source'\n",
    "# target_fname = f'/home/jitianbo/Workspace/driver_simulator_kvret/data/data_for_clm/{taskname}/test-{taskname}.target'\n",
    "\n",
    "source_fname = '/home/jitianbo/Workspace/driver_simulator_kvret/data/data_for_clm/test.source'\n",
    "target_fname = '/home/jitianbo/Workspace/driver_simulator_kvret/data/data_for_clm/test.target'\n",
    "with open(source_fname) as f:\n",
    "    source_data = f.read().strip().splitlines()\n",
    "with open(target_fname) as f:\n",
    "    target_data = f.read().strip().splitlines()\n",
    "# testset = load_dataset('text',data_files={\"test\": testset_fname})\n",
    "# testset = testset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1377cc4-2b31-43b7-acbc-4be2c1614bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path.joinpath?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d8407e-0198-4bde-98b8-c1b7892f790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeldir = '/home/jitianbo/Workspace/driver_simulator_kvret/simulator/clm-output/'.rstrip(r'/')\n",
    "# modelname = 'gpt2'\n",
    "\n",
    "# modelpath = f\"{modeldir}/{modelname}/\"\n",
    "\n",
    "# pipeline_task = \"text-generation\"\n",
    "# device = 'cuda'\n",
    "# p = pipeline(\n",
    "#     task=pipeline_task,\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     device=0,\n",
    "#     batch_size=8,\n",
    "#     max_length=512,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67293f58-33a2-46c6-83ee-6285350a3eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c43cdd-3d65-4c3d-a19f-21cbc2b64b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeldir = '/home/jitianbo/Workspace/driver_simulator_kvret/simulator/clm-output/'.rstrip(r'/')\n",
    "modelname = 'distilgpt2'\n",
    "modelpath = f\"{modeldir}/{modelname}/\"\n",
    "\n",
    "dset = 'test'\n",
    "device = 'cuda'\n",
    "model, tokenizer = load_model(modelpath,device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75340374-17cf-4414-b979-6897323ae70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_task_by_source(source_text):\n",
    "    # NUL task: assistant utterance -> assistant action\n",
    "    if source_text.endswith('[eoau]'): \n",
    "        return \"NLU\"\n",
    "    # POL task: assistant action -> driver action\n",
    "    elif source_text.endswith('[eoaa]'): \n",
    "        return \"POL\"\n",
    "    # NLG task: driver action -> driver utterance\n",
    "    else:\n",
    "        return \"NLG\"\n",
    "def get_eos_by_task(task,tokenizer):\n",
    "    eos_dict = {\n",
    "        \"NLU\": \"[eoaa]\",\n",
    "        \"POL\": \"[eoda]\",\n",
    "        \"NLG\": \"[eodu]\",\n",
    "    }\n",
    "\n",
    "    # assert task in eos_dict\n",
    "    return tokenizer.encode(eos_dict[task])[0]\n",
    "\n",
    "def get_sos_by_task(task,tokenizer):\n",
    "    sos_dict = {\n",
    "        \"NLU\": \"[soaa]\",\n",
    "        \"POL\": \"[soda]\",\n",
    "        \"NLG\": \"[sodu]\",\n",
    "    }\n",
    "    return tokenizer.encode(sos_dict[task])[0]\n",
    "\n",
    "# def decode_by_task(generated_ori,tokenizer,task):\n",
    "#     generated = generated_ori.cpu().numpy()\n",
    "#     sos_id = get_sos_by_task(task,tokenizer)\n",
    "#     eos_id = get_eos_by_task(task,tokenizer)\n",
    "#     if sos_id not in generated or eos_id not in generated:\n",
    "#         return generated\n",
    "    \n",
    "#     sos_idx = generated.index(sos_id)\n",
    "#     eos_idx = generated.index(eos_id)\n",
    "#     if sos_idx < eos_idx:\n",
    "#         to_decode = generated[:, sos_idx:eos_idx+1]\n",
    "#     else:\n",
    "#         sos_idxes = np.where(arr == 15)\n",
    "#     return to_decode\n",
    "\n",
    "def process_generated_text(result,task):\n",
    "    words = result.split()\n",
    "    eos_dict = {\n",
    "        \"NLU\": \"[eoaa]\",\n",
    "        \"POL\": \"[eoda]\",\n",
    "        \"NLG\": \"[eodu]\",\n",
    "    }\n",
    "    sos_dict = {\n",
    "        \"NLU\": \"[soaa]\",\n",
    "        \"POL\": \"[soda]\",\n",
    "        \"NLG\": \"[sodu]\",\n",
    "    }\n",
    "    sos_token = sos_dict[task]\n",
    "    eos_token = eos_dict[task]\n",
    "    if sos_token not in words or eos_token not in words:\n",
    "        return result\n",
    "    sos_id = words.index(sos_token)\n",
    "    eos_id = words.index(eos_token)\n",
    "    if sos_id < eos_id:\n",
    "        tokens = words[sos_id:eos_id+1]\n",
    "        return ' '.join(tokens)\n",
    "    words_np = np.array(words)\n",
    "    # sos_ids,*_ = np.where(words_np==sos_token)\n",
    "    eos_ids,*_ = np.where(words_np==eos_token)\n",
    "    \n",
    "    # 如果sos的第一个id比eos最后一个id还要大，说明不存在valid的数据，原样返回\n",
    "    if sos_id > eos_ids[-1]:\n",
    "        return result\n",
    "    \n",
    "    for e in eos_ids:\n",
    "        if e > sos_id:\n",
    "            break\n",
    "    eos_id = e\n",
    "    tokens = words[sos_id:eos_id+1]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def remove_special_sep_tokens(text):\n",
    "    special_sep_tokens = [\n",
    "        '[eoaa]', '[eoau]', '[eoda]', '[eodp]', '[eodu]',\n",
    "        # '[eoaa', '[eoau', '[eoda', '[eodp', '[eodu',\n",
    "     \n",
    "        '[soaa]', '[soau]', '[soda]', '[sodp]', '[sodu]',\n",
    "#         '[soaa', '[soau', '[soda', '[sodp', '[sodu',\n",
    "        \n",
    "#         '[eoa', '[eod', '[eo', '[e', '[',\n",
    "#         '[soa', '[sod', '[so', '[s', \n",
    "        \n",
    "    ]\n",
    "    text = text.replace(\"][\",\"] [\")\n",
    "    words = text.split()\n",
    "    tokens = [e for e in words if e not in special_sep_tokens]\n",
    "    tokens = [e for e in tokens if not (e.startswith(\"[\") and not e.endswith(\"]\"))]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def process_raw_result(raw_result,task):\n",
    "    return remove_special_sep_tokens(process_generated_text(raw_result,task))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00047e16-0a23-4aac-954a-b0d8f170a399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 1\n",
    "# input_text = testset[i]['text']\n",
    "# input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device).input_ids\n",
    "# # input_ids = tokenizer.encode(input_text,return_tensors=\"pt\").view(-1).to(device)\n",
    "# input_len = input_ids.shape[-1]\n",
    "# # max_length=context_length+max_len, temperature=0.7, pad_token_id=self.tokenizer.eos_token_id, eos_token_id=self.tokenizer.encode(['<eos_r>'])[0])\n",
    "# outputs = model.generate(input_ids, max_length=512,eos_token_id=self.tokenizer.encode(['[eo]'])[0])\n",
    "# outputs_len = input_ids.shape[-1]\n",
    "# # results = tokenizer.decode(outputs[:,input_len:], skip_special_tokens=False)\n",
    "# results = tokenizer.batch_decode(outputs, skip_special_tokens=False,clean_up_tokenization_spaces=False)\n",
    "# result = results[0]\n",
    "# reference = references[i]\n",
    "\n",
    "raw_result_dict = {\n",
    "    \"NLU\": [],\n",
    "    \"NLG\": [],\n",
    "    \"POL\": [],\n",
    "}\n",
    "result_dict = {\n",
    "    \"NLU\": [],\n",
    "    \"NLG\": [],\n",
    "    \"POL\": [],\n",
    "}\n",
    "target_dict = {\n",
    "    \"NLU\": [],\n",
    "    \"NLG\": [],\n",
    "    \"POL\": [],\n",
    "}\n",
    "save_dir = Path('./inference-results').absolute().resolve()\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "model_dir = save_dir.joinpath(f\"{modelname}\")\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for i,source_text in enumerate(tqdm(source_data)):\n",
    "    input_text = source_text\n",
    "    \n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device).input_ids\n",
    "    input_len = input_ids.shape[-1]\n",
    "    max_len = 80\n",
    "    # # NUL task: assistant utterance -> assistant action\n",
    "\n",
    "    # if source_text.endswith('[eoau]'): \n",
    "    #     eos_token_id = tokenizer.encode('[eoaa]')[0]\n",
    "    # # POL task: assistant action -> driver action\n",
    "    # elif source_text.endswith('[eoaa]'): \n",
    "    #     eos_token_id = tokenizer.encode('[eoda]')[0]\n",
    "    # # NLG task: driver action -> driver utterance\n",
    "    # elif source_text.endswith('[eoda]'): \n",
    "    #     eos_token_id = tokenizer.encode('[eodu]')[0]\n",
    "    \n",
    "    task = detect_task_by_source(source_text)\n",
    "    eos_token_id = get_eos_by_task(task,tokenizer)\n",
    "    \n",
    "    if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "        pad_token_id = tokenizer.eos_token_id\n",
    "    elif  tokenizer.eos_token_id is None:\n",
    "        pad_token_id = eos_token_id\n",
    "    else:\n",
    "        pad_token_id = tokenizer.pad_token_id\n",
    "    outputs = model.generate(\n",
    "        input_ids, \n",
    "        max_length=input_len+max_len,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=pad_token_id,\n",
    "        eos_token_id=eos_token_id,\n",
    "    )\n",
    "    opt = tokenizer.batch_decode(outputs, skip_special_tokens=False)[0]\n",
    "    generated = outputs[:,input_len:]\n",
    "    # to_decode = decode_by_task(generated,tokenizer,task)\n",
    "    # results = tokenizer.batch_decode(to_decode, skip_special_tokens=False,clean_up_tokenization_spaces=False)\n",
    "    results = tokenizer.batch_decode(generated, skip_special_tokens=False,clean_up_tokenization_spaces=True)\n",
    "    raw_result = results[0].strip()\n",
    "    raw_result_dict[task].append(raw_result)\n",
    "    \n",
    "    result = process_raw_result(raw_result,task)\n",
    "    result_dict[task].append(result)\n",
    "    \n",
    "    raw_target = target_data[i]\n",
    "    target = remove_special_sep_tokens(raw_target)\n",
    "    target_dict[task].append(target)\n",
    "    \n",
    "    # generated = outputs[0].numpy().tolist()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf914fc-3c01-4ac3-8de4-63800b9201db",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2560ae53-0e49-4330-9a82-7bfcb81d17f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in [\"NLU\",\"POL\",\"NLG\"]:\n",
    "    raw_result_t = raw_result_dict[t]\n",
    "    savefname = f\"{dset}-{t}.raw\"\n",
    "    savefpath = model_dir.joinpath(savefname)\n",
    "    with savefpath.open('w') as f:\n",
    "        f.writelines([f\"{e}\\n\" for e in raw_result_t])\n",
    "    \n",
    "    \n",
    "    result_t = result_dict[t]\n",
    "    savefname = f\"{dset}-{t}.result\"\n",
    "    savefpath = model_dir.joinpath(savefname)\n",
    "    with savefpath.open('w') as f:\n",
    "        f.writelines([f\"{e}\\n\" for e in result_t])\n",
    "    \n",
    "    target_t = target_dict[t]\n",
    "    savefname = f\"{dset}-{t}.target\"\n",
    "    savefpath = model_dir.joinpath(savefname)\n",
    "    with savefpath.open('w') as f:\n",
    "        f.writelines([f\"{e}\\n\" for e in target_t])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d32dd7-9a7b-4000-bca7-e2f5f33a2f90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
