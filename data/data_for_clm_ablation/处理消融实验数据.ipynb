{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd6e9a89-b584-401f-8877-654278ffbfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eefc212d-5128-4d6a-bfb0-ba480fb78e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\"NLU\", \"POL\", \"NLG\"]\n",
    "special_sep_tokens = [\n",
    "    \"[eoaa]\",\n",
    "    \"[eoau]\",\n",
    "    \"[eoda]\",\n",
    "    \"[eodp]\",\n",
    "    \"[eodu]\",\n",
    "    \"[soaa]\",\n",
    "    \"[soau]\",\n",
    "    \"[soda]\",\n",
    "    \"[sodp]\",\n",
    "    \"[sodu]\",\n",
    "]\n",
    "basedir = Path('/home/jitianbo/Workspace/driver_simulator_kvret/data/data_for_clm_ablation/')\n",
    "csvdir = Path('/home/jitianbo/Workspace/driver_simulator_kvret/data/data_with_dp/')\n",
    "\n",
    "\n",
    "modes = [\"train\", \"test\", \"dev\"]\n",
    "csv_data = {\n",
    "    mode: pd.read_csv(csvdir.joinpath(f\"{mode}.csv\"))\n",
    "    for mode in modes\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def remove_ss_token(txt):\n",
    "    tokens = txt.split()\n",
    "    tokens = [e for e in tokens if e not in special_sep_tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def remove_dup_space(txt):\n",
    "    tokens = txt.strip().split()\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ccf656-4fc4-49f3-9806-6852eb267e19",
   "metadata": {},
   "source": [
    "# 去除history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97287380-b7a2-4cd8-a364-43406bba053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "savebasedir = basedir.joinpath(\"history\")\n",
    "savebasedir.mkdir(exist_ok=True)\n",
    "\n",
    "sp_token_fpath = savebasedir.joinpath(\"additional_special_tokens.json\")\n",
    "\n",
    "sp_tokens_to_add = {\n",
    "    \"additional_special_tokens\": [\n",
    "        \"[sodp]\", \"[eodp]\", # driver profile\n",
    "        \"[soau]\", \"[eoau]\", # assistant utterance\n",
    "        \"[soaa]\", \"[eoaa]\", # assistant action\n",
    "        \"[soda]\", \"[eoda]\", # driver action\n",
    "        \"[sodu]\", \"[eodu]\", # driver utterance         \n",
    "        \"[address]\", \"[agenda]\", \"[date]\", \"[distance]\", \"[event]\", \"[friday]\", \n",
    "        \"[greeting]\", \"[location]\", \"[monday]\", \"[party]\", \"[poi]\", \"[poi_type]\", \n",
    "        \"[room]\", \"[saturday]\", \"[sunday]\", \"[thursday]\", \"[time]\", \"[today]\", \"[traffic_info]\", \n",
    "        \"[tuesday]\", \"[weather_attribute]\", \"[wednesday]\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "with sp_token_fpath.open('w') as f:\n",
    "    json.dump(sp_tokens_to_add,f,indent=2)\n",
    "\n",
    "\n",
    "for mode in modes:\n",
    "    txt_lines = []\n",
    "    sour_lines = []\n",
    "    targ_lines = []\n",
    "    task_txt_lines = {e:[] for e in tasks}\n",
    "    task_sour_lines = {e:[] for e in tasks}\n",
    "    task_targ_lines = {e:[] for e in tasks}\n",
    "    pd_data = csv_data[mode]\n",
    "    for idx,row in pd_data.iterrows():\n",
    "        cur_task = row['task']\n",
    "        if cur_task == 'DST':\n",
    "            cur_task = 'POL'\n",
    "        if cur_task == 'NLU' and row['turn_id'] == 0:\n",
    "            continue\n",
    "        # history = row['history']\n",
    "        # if pd.isna(history):\n",
    "        #     history = \"\"\n",
    "        input_ = row['input']\n",
    "        profile = row['profile']\n",
    "        target = row['target']\n",
    "        \n",
    "        # sour_line = f\"{profile} {history} {input_}\"\n",
    "        sour_line = f\"{profile} {input_}\"\n",
    "        targ_line = f\"{target}\"\n",
    "        txt_line = f\"{sour_line} {targ_line}\"\n",
    "\n",
    "\n",
    "        \n",
    "        txt_line = remove_dup_space(txt_line)\n",
    "        sour_line = remove_dup_space(sour_line)\n",
    "        targ_line = remove_dup_space(targ_line)\n",
    "        # 去除空数据\n",
    "        # if sour_line == '[soda] [eoda]':\n",
    "        #     continue\n",
    "        \n",
    "        txt_lines.append(txt_line)\n",
    "        sour_lines.append(sour_line)\n",
    "        targ_lines.append(targ_line)\n",
    "        \n",
    "        task_txt_lines[cur_task].append(txt_line)\n",
    "        task_sour_lines[cur_task].append(sour_line)\n",
    "        task_targ_lines[cur_task].append(targ_line)\n",
    "        \n",
    "    txt_path = savebasedir.joinpath(f\"{mode}.txt\")\n",
    "    with txt_path.open('w') as f:\n",
    "        f.writelines( [f\"{e}\\n\" for e in txt_lines])\n",
    "        \n",
    "    sour_path = savebasedir.joinpath(f\"{mode}.source\")\n",
    "    with sour_path.open('w') as f:\n",
    "        f.writelines( [f\"{e}\\n\" for e in sour_lines])\n",
    "    \n",
    "    targ_path = savebasedir.joinpath(f\"{mode}.target\")\n",
    "    with targ_path.open('w') as f:\n",
    "        f.writelines( [f\"{e}\\n\" for e in targ_lines])\n",
    "    \n",
    "    for task in tasks:\n",
    "        task_savebasedir = savebasedir.joinpath(task)\n",
    "        task_savebasedir.mkdir(exist_ok=True)\n",
    "        \n",
    "        cur_task_txt_lines = task_txt_lines[task]\n",
    "        cur_task_sour_lines = task_sour_lines[task]\n",
    "        cur_task_targ_lines = task_sour_lines[task]\n",
    "        \n",
    "        task_txt_path = task_savebasedir.joinpath(f\"{mode}-{task}.txt\")\n",
    "        with task_txt_path.open('w') as f:\n",
    "            f.writelines( [f\"{e}\\n\" for e in cur_task_txt_lines])\n",
    "\n",
    "        task_sour_path = task_savebasedir.joinpath(f\"{mode}-{task}.source\")\n",
    "        with task_sour_path.open('w') as f:\n",
    "            f.writelines( [f\"{e}\\n\" for e in cur_task_sour_lines])\n",
    "\n",
    "        task_targ_path = task_savebasedir.joinpath(f\"{mode}-{task}.target\")\n",
    "        with task_targ_path.open('w') as f:\n",
    "            f.writelines( [f\"{e}\\n\" for e in cur_task_targ_lines])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85550eb8-167e-4690-ac56-705468c292ff",
   "metadata": {},
   "source": [
    "# 去除profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "852905f5-1fb3-493f-b62e-0d767367e310",
   "metadata": {},
   "outputs": [],
   "source": [
    "savebasedir = basedir.joinpath(\"profile\")\n",
    "savebasedir.mkdir(exist_ok=True)\n",
    "\n",
    "sp_token_fpath = savebasedir.joinpath(\"additional_special_tokens.json\")\n",
    "\n",
    "# 没有 driver profile\n",
    "sp_tokens_to_add = {\n",
    "    \"additional_special_tokens\": [\n",
    "        \"[sodp]\", \"[eodp]\", # driver profile\n",
    "        \"[soau]\", \"[eoau]\", # assistant utterance\n",
    "        \"[soaa]\", \"[eoaa]\", # assistant action\n",
    "        \"[soda]\", \"[eoda]\", # driver action\n",
    "        \"[sodu]\", \"[eodu]\", # driver utterance         \n",
    "        \"[address]\", \"[agenda]\", \"[date]\", \"[distance]\", \"[event]\", \"[friday]\", \n",
    "        \"[greeting]\", \"[location]\", \"[monday]\", \"[party]\", \"[poi]\", \"[poi_type]\", \n",
    "        \"[room]\", \"[saturday]\", \"[sunday]\", \"[thursday]\", \"[time]\", \"[today]\", \"[traffic_info]\", \n",
    "        \"[tuesday]\", \"[weather_attribute]\", \"[wednesday]\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "with sp_token_fpath.open('w') as f:\n",
    "    json.dump(sp_tokens_to_add,f,indent=2)\n",
    "\n",
    "for mode in modes:\n",
    "    txt_lines = []\n",
    "    sour_lines = []\n",
    "    targ_lines = []\n",
    "    task_txt_lines = {e:[] for e in tasks}\n",
    "    task_sour_lines = {e:[] for e in tasks}\n",
    "    task_targ_lines = {e:[] for e in tasks}\n",
    "    pd_data = csv_data[mode]\n",
    "    for idx,row in pd_data.iterrows():\n",
    "        cur_task = row['task']\n",
    "        if cur_task == 'DST':\n",
    "            cur_task = 'POL'\n",
    "        if cur_task == 'NLU' and row['turn_id'] == 0:\n",
    "            continue\n",
    "        history = row['history']\n",
    "        if pd.isna(history):\n",
    "            history = \"\"\n",
    "        input_ = row['input']\n",
    "        profile = row['profile']\n",
    "        target = row['target']\n",
    "        \n",
    "        # sour_line = f\"{profile} {history} {input_}\"\n",
    "        sour_line = f\"{history} {input_}\"\n",
    "        targ_line = f\"{target}\"\n",
    "        txt_line = f\"{sour_line} {targ_line}\"\n",
    "        \n",
    "        txt_line = remove_dup_space(txt_line)\n",
    "        sour_line = remove_dup_space(sour_line)\n",
    "        targ_line = remove_dup_space(targ_line)\n",
    "        \n",
    "        # 去除空数据\n",
    "        # if sour_line == '[soau] [eoau] [soaa] [eoaa]' and mode == \"train\":\n",
    "        # if sour_line == '[soau] [eoau] [soaa] [eoaa]':\n",
    "            # continue\n",
    "        \n",
    "        txt_lines.append(txt_line)\n",
    "        sour_lines.append(sour_line)\n",
    "        targ_lines.append(targ_line)\n",
    "        \n",
    "        task_txt_lines[cur_task].append(txt_line)\n",
    "        task_sour_lines[cur_task].append(sour_line)\n",
    "        task_targ_lines[cur_task].append(targ_line)\n",
    "        \n",
    "    txt_path = savebasedir.joinpath(f\"{mode}.txt\")\n",
    "    with txt_path.open('w') as f:\n",
    "        f.writelines( [f\"{e}\\n\" for e in txt_lines])\n",
    "        \n",
    "    sour_path = savebasedir.joinpath(f\"{mode}.source\")\n",
    "    with sour_path.open('w') as f:\n",
    "        f.writelines( [f\"{e}\\n\" for e in sour_lines])\n",
    "    \n",
    "    targ_path = savebasedir.joinpath(f\"{mode}.target\")\n",
    "    with targ_path.open('w') as f:\n",
    "        f.writelines( [f\"{e}\\n\" for e in targ_lines])\n",
    "    \n",
    "    for task in tasks:\n",
    "        task_savebasedir = savebasedir.joinpath(task)\n",
    "        task_savebasedir.mkdir(exist_ok=True)\n",
    "        \n",
    "        cur_task_txt_lines = task_txt_lines[task]\n",
    "        cur_task_sour_lines = task_sour_lines[task]\n",
    "        cur_task_targ_lines = task_sour_lines[task]\n",
    "        \n",
    "        task_txt_path = task_savebasedir.joinpath(f\"{mode}-{task}.txt\")\n",
    "        with task_txt_path.open('w') as f:\n",
    "            f.writelines( [f\"{e}\\n\" for e in cur_task_txt_lines])\n",
    "\n",
    "        task_sour_path = task_savebasedir.joinpath(f\"{mode}-{task}.source\")\n",
    "        with task_sour_path.open('w') as f:\n",
    "            f.writelines( [f\"{e}\\n\" for e in cur_task_sour_lines])\n",
    "\n",
    "        task_targ_path = task_savebasedir.joinpath(f\"{mode}-{task}.target\")\n",
    "        with task_targ_path.open('w') as f:\n",
    "            f.writelines( [f\"{e}\\n\" for e in cur_task_targ_lines])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1f5d67-0e74-4b3c-9d95-c1d6724f0763",
   "metadata": {},
   "source": [
    "# 去除history和profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38979a42-0a32-4d9e-b03e-66eea68ff435",
   "metadata": {},
   "outputs": [],
   "source": [
    "savebasedir = basedir.joinpath(\"history-profile\")\n",
    "# savebasedir = basedir.joinpath(\"history-profile-2\")\n",
    "savebasedir.mkdir(exist_ok=True)\n",
    "\n",
    "sp_token_fpath = savebasedir.joinpath(\"additional_special_tokens.json\")\n",
    "\n",
    "# 没有 driver profile\n",
    "sp_tokens_to_add = {\n",
    "    \"additional_special_tokens\": [\n",
    "        \"[sodp]\", \"[eodp]\", # driver profile\n",
    "        \"[soau]\", \"[eoau]\", # assistant utterance\n",
    "        \"[soaa]\", \"[eoaa]\", # assistant action\n",
    "        \"[soda]\", \"[eoda]\", # driver action\n",
    "        \"[sodu]\", \"[eodu]\", # driver utterance         \n",
    "        \"[address]\", \"[agenda]\", \"[date]\", \"[distance]\", \"[event]\", \"[friday]\", \n",
    "        \"[greeting]\", \"[location]\", \"[monday]\", \"[party]\", \"[poi]\", \"[poi_type]\", \n",
    "        \"[room]\", \"[saturday]\", \"[sunday]\", \"[thursday]\", \"[time]\", \"[today]\", \"[traffic_info]\", \n",
    "        \"[tuesday]\", \"[weather_attribute]\", \"[wednesday]\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "with sp_token_fpath.open('w') as f:\n",
    "    json.dump(sp_tokens_to_add,f,indent=2)\n",
    "\n",
    "\n",
    "for mode in modes:\n",
    "    txt_lines = []\n",
    "    sour_lines = []\n",
    "    targ_lines = []\n",
    "    task_txt_lines = {e:[] for e in tasks}\n",
    "    task_sour_lines = {e:[] for e in tasks}\n",
    "    task_targ_lines = {e:[] for e in tasks}\n",
    "    pd_data = csv_data[mode]\n",
    "    for idx,row in pd_data.iterrows():\n",
    "        cur_task = row['task']\n",
    "        if cur_task == 'DST':\n",
    "            cur_task = 'POL'\n",
    "        if cur_task == 'NLU' and row['turn_id'] == 0:\n",
    "            continue\n",
    "        history = row['history']\n",
    "        if pd.isna(history):\n",
    "            history = \"\"\n",
    "        input_ = row['input']\n",
    "        profile = row['profile']\n",
    "        target = row['target']\n",
    "        \n",
    "        # sour_line = f\"{profile} {history} {input_}\"\n",
    "        sour_line = f\"{input_}\"\n",
    "        targ_line = f\"{target}\"\n",
    "        txt_line = f\"{sour_line} {targ_line}\"\n",
    "        \n",
    "        txt_line = remove_dup_space(txt_line)\n",
    "        sour_line = remove_dup_space(sour_line)\n",
    "        targ_line = remove_dup_space(targ_line)\n",
    "        \n",
    "        # 去除空数据\n",
    "        # if sour_line == '[soau] [eoau] [soaa] [eoaa]' and mode == \"train\":\n",
    "        # if sour_line == '[soau] [eoau] [soaa] [eoaa]':\n",
    "        #     continue\n",
    "        \n",
    "        \n",
    "        txt_lines.append(txt_line)\n",
    "        sour_lines.append(sour_line)\n",
    "        targ_lines.append(targ_line)\n",
    "        \n",
    "        task_txt_lines[cur_task].append(txt_line)\n",
    "        task_sour_lines[cur_task].append(sour_line)\n",
    "        task_targ_lines[cur_task].append(targ_line)\n",
    "        \n",
    "    txt_path = savebasedir.joinpath(f\"{mode}.txt\")\n",
    "    with txt_path.open('w') as f:\n",
    "        f.writelines( [f\"{e}\\n\" for e in txt_lines])\n",
    "        \n",
    "    sour_path = savebasedir.joinpath(f\"{mode}.source\")\n",
    "    with sour_path.open('w') as f:\n",
    "        f.writelines( [f\"{e}\\n\" for e in sour_lines])\n",
    "    \n",
    "    targ_path = savebasedir.joinpath(f\"{mode}.target\")\n",
    "    with targ_path.open('w') as f:\n",
    "        f.writelines( [f\"{e}\\n\" for e in targ_lines])\n",
    "    \n",
    "    for task in tasks:\n",
    "        task_savebasedir = savebasedir.joinpath(task)\n",
    "        task_savebasedir.mkdir(exist_ok=True)\n",
    "        \n",
    "        cur_task_txt_lines = task_txt_lines[task]\n",
    "        cur_task_sour_lines = task_sour_lines[task]\n",
    "        cur_task_targ_lines = task_sour_lines[task]\n",
    "        \n",
    "        task_txt_path = task_savebasedir.joinpath(f\"{mode}-{task}.txt\")\n",
    "        with task_txt_path.open('w') as f:\n",
    "            f.writelines( [f\"{e}\\n\" for e in cur_task_txt_lines])\n",
    "\n",
    "        task_sour_path = task_savebasedir.joinpath(f\"{mode}-{task}.source\")\n",
    "        with task_sour_path.open('w') as f:\n",
    "            f.writelines( [f\"{e}\\n\" for e in cur_task_sour_lines])\n",
    "\n",
    "        task_targ_path = task_savebasedir.joinpath(f\"{mode}-{task}.target\")\n",
    "        with task_targ_path.open('w') as f:\n",
    "            f.writelines( [f\"{e}\\n\" for e in cur_task_targ_lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00c5140-205b-42b2-bb27-40c907412310",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
