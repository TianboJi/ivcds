{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd6e9a89-b584-401f-8877-654278ffbfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eefc212d-5128-4d6a-bfb0-ba480fb78e3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (882115108.py, line 59)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [21]\u001b[0;36m\u001b[0m\n\u001b[0;31m    tokens = [special_sep_tokens for e in tokens if e in special_sep_tokens else e]\u001b[0m\n\u001b[0m                                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "tasks = [\"NLU\", \"POL\", \"NLG\"]\n",
    "special_sep_tokens = [\n",
    "    \"[eoaa]\",\n",
    "    \"[eoau]\",\n",
    "    \"[eoda]\",\n",
    "    \"[eodp]\",\n",
    "    \"[eodu]\",\n",
    "    \"[soaa]\",\n",
    "    \"[soau]\",\n",
    "    \"[soda]\",\n",
    "    \"[sodp]\",\n",
    "    \"[sodu]\",\n",
    "]\n",
    "basedir = Path('/home/jitianbo/Workspace/driver_simulator_kvret/data/data_for_clm/')\n",
    "csvdir = Path('/home/jitianbo/Workspace/driver_simulator_kvret/data/data_with_dp/')\n",
    "\n",
    "\n",
    "modes = [\"train\", \"test\", \"dev\"]\n",
    "csv_data = {\n",
    "    mode: pd.read_csv(csvdir.joinpath(f\"{mode}.csv\"))\n",
    "    for mode in modes\n",
    "}\n",
    "\n",
    "models = [\n",
    "    (\n",
    "        \"bert-large-uncased\", \n",
    "        \"bert\", \n",
    "        AutoTokenizer.from_pretrained(\"bert-large-uncased\"),\n",
    "    ),\n",
    "    (\n",
    "        \"roberta-large\", \n",
    "        \"roberta\",\n",
    "        AutoTokenizer.from_pretrained(\"roberta-large\"),\n",
    "    ),\n",
    "    \n",
    "]\n",
    "\n",
    "def remove_ss_token(txt):\n",
    "    tokens = txt.split()\n",
    "    tokens = [e for e in tokens if e not in special_sep_tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd7af860-9754-4601-b129-b2799345a95c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[1][2].pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c6bf699-82f9-4f28-b346-edb4e5bdf3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_ss_token_in_history(txt,sep_token):\n",
    "    # special_sep_tokens\n",
    "    special_sep_tokens = [\n",
    "        \"[eoaa]\",\n",
    "        \"[eoau]\",\n",
    "        \"[eoda]\",\n",
    "        \"[eodp]\",\n",
    "        \"[eodu]\",\n",
    "        \"[soaa]\",\n",
    "        \"[soau]\",\n",
    "        \"[soda]\",\n",
    "        \"[sodp]\",\n",
    "        \"[sodu]\",\n",
    "    ]\n",
    "    tokens = txt.split()\n",
    "    tokens = [e for e in tokens if e not in special_sep_tokens or not e.startswith(\"[so\")]\n",
    "    tokens = [sep_token if e in special_sep_tokens else e for e in tokens ]\n",
    "    \n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0712801a-1c35-4b49-a9af-905ab47e816b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # models = {\n",
    "# #     \"bert-large-uncased\": \"bert\",\n",
    "# #     \"roberta-large\": \"roberta\",\n",
    "# # }\n",
    "\n",
    "\n",
    "#     # tokenizer = AutoTokenizer.from_pretrained(modelname)\n",
    "# for task in tasks:\n",
    "#     dirpath = basedir.joinpath(task)\n",
    "#     task_str = \"DST\" if task == \"POL\" else task\n",
    "    \n",
    "#     for mode in modes:\n",
    "#         pd_data = csv_data[mode]\n",
    "        \n",
    "#         pd_data = pd_data[pd_data['task']]\n",
    "#         source_fname = f\"{mode}-{task}.source\"\n",
    "#         source_fpath = dirpath.joinpath(source_fname)\n",
    "#         with source_fpath.open() as f:\n",
    "#             sour_data = f.read().strip().splitlines()\n",
    "#         sour_data = [remove_ss_token(e) for e in sour_data]\n",
    "        \n",
    "#         target_fname = f\"{mode}-{task}.target\"\n",
    "#         target_fpath = dirpath.joinpath(target_fname)\n",
    "#         with target_fpath.open() as f:\n",
    "#             targ_data = f.read().strip().splitlines()\n",
    "#         targ_data = [remove_ss_token(e) for e in targ_data]\n",
    "    \n",
    "#         for modelname, savename, tokenizer in models:\n",
    "#             savedir = dirpath.joinpath(savename)\n",
    "#             savedir.mkdir(exist_ok=True)\n",
    "#             sep_token = tokenizer.sep_token\n",
    "            \n",
    "#             train_lines = []\n",
    "#             sour_lines = []\n",
    "#             targ_lines = []\n",
    "#             for sour,targ in zip(sour_data,targ_data):\n",
    "#                 if sour == \"\" or targ == \"\":\n",
    "#                     continue\n",
    "#                 train_txt = f\"{sour} {sep_token} {targ} {sep_token}\\n\"\n",
    "#                 train_lines.append(train_txt)\n",
    "                \n",
    "#                 sour_txt = f\"{sour} {sep_token}\\n\"\n",
    "#                 sour_lines.append(sour_txt)\n",
    "                \n",
    "#                 targ_txt = f\"{targ}\\n\"\n",
    "#                 targ_lines.append(targ_txt)\n",
    "            \n",
    "#             train_fname = f\"{mode}.txt\"\n",
    "#             train_fpath = savedir.joinpath(train_fname)\n",
    "#             with train_fpath.open('w') as f:\n",
    "#                 f.writelines(train_lines)\n",
    "            \n",
    "#             sour_fname = f\"{mode}.source\"\n",
    "#             sour_fpath = savedir.joinpath(sour_fname)\n",
    "#             with sour_fpath.open('w') as f:\n",
    "#                 f.writelines(sour_lines)\n",
    "            \n",
    "#             targ_fname = f\"{mode}.target\"\n",
    "#             targ_fpath = savedir.joinpath(targ_fname)\n",
    "#             with targ_fpath.open('w') as f:\n",
    "#                 f.writelines(targ_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97287380-b7a2-4cd8-a364-43406bba053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in tasks:\n",
    "    dirpath = basedir.joinpath(task)\n",
    "    task_str = \"DST\" if task == \"POL\" else task\n",
    "    \n",
    "    for mode in modes:\n",
    "        pd_data = csv_data[mode]\n",
    "        \n",
    "        cur_pd = pd_data[pd_data['task']==task_str]\n",
    "        \n",
    "        for modelname, savename, tokenizer in models:\n",
    "            savedir = dirpath.joinpath(savename)\n",
    "            savedir.mkdir(exist_ok=True)\n",
    "            sep_token = tokenizer.sep_token\n",
    "            \n",
    "            train_lines = []\n",
    "            sour_lines = []\n",
    "            targ_lines = []\n",
    "            for index,row in cur_pd.iterrows():\n",
    "                history = row['history']\n",
    "                if pd.isna(history):\n",
    "                    history = \"\"\n",
    "                input_ = row['input']\n",
    "                profile = row['profile']\n",
    "                target = row['target']\n",
    "                    \n",
    "                history = remove_ss_token(history)\n",
    "                input_ = remove_ss_token(input_)\n",
    "                profile = remove_ss_token(profile)\n",
    "                target = remove_ss_token(target)\n",
    "                if f\"{history}{input_}\" == \"\" or target==\"\":\n",
    "                    continue\n",
    "                \n",
    "                if task == \"POL\":\n",
    "                    sour_txt = f\"{profile} {sep_token} {history} {sep_token} {input_} {sep_token}\"\n",
    "                elif history != \"\":\n",
    "                    sour_txt = f\"{history} {sep_token} {input_} {sep_token}\"\n",
    "                else:\n",
    "                    sour_txt = f\"{input_} {sep_token}\"\n",
    "                train_txt = f\"{sour_txt} {target} {sep_token}\"\n",
    "                targ_txt = f\"{target}\"\n",
    "                \n",
    "\n",
    "                train_lines.append(f\"{train_txt}\\n\")\n",
    "                sour_lines.append(f\"{sour_txt}\\n\")\n",
    "                targ_lines.append(f\"{targ_txt}\\n\")\n",
    "            \n",
    "#             train_fname = f\"{mode}.txt\"\n",
    "#             train_fpath = savedir.joinpath(train_fname)\n",
    "#             with train_fpath.open('w') as f:\n",
    "#                 f.writelines(train_lines)\n",
    "            \n",
    "#             sour_fname = f\"{mode}.source\"\n",
    "#             sour_fpath = savedir.joinpath(sour_fname)\n",
    "#             with sour_fpath.open('w') as f:\n",
    "#                 f.writelines(sour_lines)\n",
    "            \n",
    "#             targ_fname = f\"{mode}.target\"\n",
    "#             targ_fpath = savedir.joinpath(targ_fname)\n",
    "#             with targ_fpath.open('w') as f:\n",
    "#                 f.writelines(targ_lines)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
